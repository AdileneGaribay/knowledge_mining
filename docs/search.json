[
  {
    "objectID": "Assignment 2/assignment_2.html",
    "href": "Assignment 2/assignment_2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "‚ÄúConduct a 2,000-word structured systematic literature review on the applications of data mining and machine learning in real-world domains. Include a methodology section, synthesize key findings, identify trends and gaps, and propose one testable hypothesis. Use an academic tone and emulate systematic review standards‚Äù\n\n\n\nUsing the original prompt on ChatGPT yields a response that has no sources listed. It is also very concise in its findings of machine learning in real-world domains. For the next prompt, I requested that the citations be added ‚Äú‚ÄúProvide sources in APA‚Äù‚Äú. Then, I inputted the following prompt,‚ÄùProvide summaries for the findings section. Avoid using bullet points.‚Äù. This prompt yielded summaries for each finding as opposed to giving oversimplified bullet points of the findings. ChatGPT does include gaps in the research & proposes a testable hypothesis. I checked the citations provided on Google Scholar and they were all existing works. Thus, there was no ‚Äúhallucinated‚Äù reference.\n\n\n\n\n\nGemini listed sources, however, they are not in the correct formatt. It also exludes in-text citation. In contrast to ChatGPT, it offers more analysis on the applications across different domains.\n\n\n\n\n\nClaude offers the most comprehensive and structurally accurate response. It includes sources, but not in the correct formatt nor does it offer in-text citations.\n\n##Updated Prompt:I am a Data Science Graduate Student interested in data mining and machine learning. Conduct a 2,000-word structured systematic literature review on the applications of data mining and machine learning in real-world domains. Include a methodology section, synthesize key findings, identify trends and gaps, and propose one testable hypothesis. Use an academic tone and emulate systematic review standards. Use Actual reference details should be drawn from the literature search conducted in the methodology. Make this in APSA formatted reference list, and with in-text citations integrated into a full paper draft. Include all primary studies selected for analysis in reference list. Convert this draft into APA"
  },
  {
    "objectID": "Assignment 2/assignment_2.html#prompt",
    "href": "Assignment 2/assignment_2.html#prompt",
    "title": "Assignment 2",
    "section": "",
    "text": "‚ÄúConduct a 2,000-word structured systematic literature review on the applications of data mining and machine learning in real-world domains. Include a methodology section, synthesize key findings, identify trends and gaps, and propose one testable hypothesis. Use an academic tone and emulate systematic review standards‚Äù\n\n\n\nUsing the original prompt on ChatGPT yields a response that has no sources listed. It is also very concise in its findings of machine learning in real-world domains. For the next prompt, I requested that the citations be added ‚Äú‚ÄúProvide sources in APA‚Äù‚Äú. Then, I inputted the following prompt,‚ÄùProvide summaries for the findings section. Avoid using bullet points.‚Äù. This prompt yielded summaries for each finding as opposed to giving oversimplified bullet points of the findings. ChatGPT does include gaps in the research & proposes a testable hypothesis. I checked the citations provided on Google Scholar and they were all existing works. Thus, there was no ‚Äúhallucinated‚Äù reference.\n\n\n\n\n\nGemini listed sources, however, they are not in the correct formatt. It also exludes in-text citation. In contrast to ChatGPT, it offers more analysis on the applications across different domains.\n\n\n\n\n\nClaude offers the most comprehensive and structurally accurate response. It includes sources, but not in the correct formatt nor does it offer in-text citations.\n\n##Updated Prompt:I am a Data Science Graduate Student interested in data mining and machine learning. Conduct a 2,000-word structured systematic literature review on the applications of data mining and machine learning in real-world domains. Include a methodology section, synthesize key findings, identify trends and gaps, and propose one testable hypothesis. Use an academic tone and emulate systematic review standards. Use Actual reference details should be drawn from the literature search conducted in the methodology. Make this in APSA formatted reference list, and with in-text citations integrated into a full paper draft. Include all primary studies selected for analysis in reference list. Convert this draft into APA"
  },
  {
    "objectID": "Assignment 2/assignment_2.html#prompt-to-combine-the-reponses-using-these-drafts-from-three-ai-models-produce-a-2000-word-structured-systematic-literature-review-on-data-mining-and-machine-learning-applications.-combine-the-strongest-methodology-findings-trends-gaps-and-hypothesis-into-a-cohesive-academically-sound-document.",
    "href": "Assignment 2/assignment_2.html#prompt-to-combine-the-reponses-using-these-drafts-from-three-ai-models-produce-a-2000-word-structured-systematic-literature-review-on-data-mining-and-machine-learning-applications.-combine-the-strongest-methodology-findings-trends-gaps-and-hypothesis-into-a-cohesive-academically-sound-document.",
    "title": "Assignment 2",
    "section": "Prompt to combine the reponses: Using these drafts from three AI models, produce a 2,000-word structured systematic literature review on data mining and machine learning applications. Combine the strongest methodology, findings, trends, gaps, and hypothesis into a cohesive, academically sound document.",
    "text": "Prompt to combine the reponses: Using these drafts from three AI models, produce a 2,000-word structured systematic literature review on data mining and machine learning applications. Combine the strongest methodology, findings, trends, gaps, and hypothesis into a cohesive, academically sound document."
  },
  {
    "objectID": "Assignment 2/assignment_2.html#results",
    "href": "Assignment 2/assignment_2.html#results",
    "title": "Assignment 2",
    "section": "Results:",
    "text": "Results:\n\nCode\ncat('&lt;a href=\"systematic_literature_review_Final.pdf\" target=\"_blank\"&gt;üìÑ Download Systematic Literature Review&lt;/a&gt;')\n\nüìÑ Download Systematic Literature Review"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Adilene Garibay",
    "section": "",
    "text": "Welcome! This site serves as a digital portfolio for my graduate work product. Here, I document my journey through the Social Data Analytics and Research program, focusing on the intersection of data science, social patterns, and actionable insights.\nAbout Me\nI am a graduate student currently exploring the world of Knowledge Mining, NLP, and predictive modeling. My goal is to leverage data-driven methodologies to solve complex social problems.\nFeatured Coursework\n\nKnowledge Mining\nExploring the DIKW hierarchy, neural extraction methods, and the ‚ÄúAI for Science‚Äù paradigm to transform raw data into actionable insights.\n\n\nData Collection Methods\nFocusing on the systematic gathering of social data, survey design, and ensuring data integrity for robust research outcomes.\n\n\nData Visualization\nTranslating complex data sets into intuitive, high-impact visual narratives to aid in decision-making and public communication.\nContact\n\nGitHub: AdileneGaribay\nEmail: dal661311@utdallas.edu\n\n\nThis website was developed as part of my graduate coursework. Technical assistance and structural guidance were provided by Gemini 1.5 Flash (LLM)."
  },
  {
    "objectID": "assignment_1_files/Lab01.html",
    "href": "assignment_1_files/Lab01.html",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "Code\nx &lt;- c(1,3,2,5)\nx\n\n\n[1] 1 3 2 5\n\n\nCode\nx = c(1,6,2)\nx\n\n\n[1] 1 6 2\n\n\nCode\ny = c(1,4,3)\n\n\n\n\n\n\n\nCode\nlength(x)  # What does length() do?\n\n\n[1] 3\n\n\nCode\nlength(y)\n\n\n[1] 3\n\n\n\n\n\n\n\nCode\nx+y\n\n\n[1]  2 10  5\n\n\nCode\nls() # List objects in the environment\n\n\n[1] \"pandoc_dir\"      \"quarto_bin_path\" \"x\"               \"y\"              \n\n\nCode\nrm(x,y) # Remove objects\nls()\n\n\n[1] \"pandoc_dir\"      \"quarto_bin_path\"\n\n\nCode\nrm(list=ls()) # Danger! What does this do?  Not recommended!\n\n\n\n\n\n\n\nCode\n?matrix\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\nCode\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\n\nCode\nsqrt(x) # What does x look like?\n\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\n\nCode\nx\n\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\nCode\nx^2\n\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\n\nCode\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n\n[1] 0.9928226\n\n\nCode\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\n\nCode\nset.seed(3) # Try different seeds?\ny=rnorm(100)\n\n\n\n\n\n\n\nCode\nmean(y)\n\n\n[1] 0.01103557\n\n\nCode\nvar(y)\n\n\n[1] 0.7328675\n\n\nCode\nsqrt(var(y))\n\n\n[1] 0.8560768\n\n\nCode\nsd(y)\n\n\n[1] 0.8560768\n\n\n\n\n\n\n\nCode\nx=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\n\n\n\n\n\n\nCode\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\n\n\n\n\n\n\nCode\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\n\n\n\n\n\n\nCode\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\n\nquartz_off_screen \n                2 \n\n\nCode\nplot(x,y,pch=20, col=\"forestgreen\")\n\n\n\n\n\n\n\n\n\nCode\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nCode\nx=1:10\nx\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nCode\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "assignment_1_files/Lab01.html#create-object-using-the-assignment-operator--",
    "href": "assignment_1_files/Lab01.html#create-object-using-the-assignment-operator--",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "Code\nx &lt;- c(1,3,2,5)\nx\n\n\n[1] 1 3 2 5\n\n\nCode\nx = c(1,6,2)\nx\n\n\n[1] 1 6 2\n\n\nCode\ny = c(1,4,3)"
  },
  {
    "objectID": "assignment_1_files/Lab01.html#using-function",
    "href": "assignment_1_files/Lab01.html#using-function",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "Code\nlength(x)  # What does length() do?\n\n\n[1] 3\n\n\nCode\nlength(y)\n\n\n[1] 3"
  },
  {
    "objectID": "assignment_1_files/Lab01.html#using---operators",
    "href": "assignment_1_files/Lab01.html#using---operators",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "Code\nx+y\n\n\n[1]  2 10  5\n\n\nCode\nls() # List objects in the environment\n\n\n[1] \"pandoc_dir\"      \"quarto_bin_path\" \"x\"               \"y\"              \n\n\nCode\nrm(x,y) # Remove objects\nls()\n\n\n[1] \"pandoc_dir\"      \"quarto_bin_path\"\n\n\nCode\nrm(list=ls()) # Danger! What does this do?  Not recommended!"
  },
  {
    "objectID": "assignment_1_files/Lab01.html#matrix-operations",
    "href": "assignment_1_files/Lab01.html#matrix-operations",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "Code\n?matrix\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\nCode\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\n\nCode\nsqrt(x) # What does x look like?\n\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\n\nCode\nx\n\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\nCode\nx^2\n\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\n\nCode\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n\n[1] 0.9928226\n\n\nCode\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\n\nCode\nset.seed(3) # Try different seeds?\ny=rnorm(100)"
  },
  {
    "objectID": "assignment_1_files/Lab01.html#simple-descriptive-statistics-base",
    "href": "assignment_1_files/Lab01.html#simple-descriptive-statistics-base",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "Code\nmean(y)\n\n\n[1] 0.01103557\n\n\nCode\nvar(y)\n\n\n[1] 0.7328675\n\n\nCode\nsqrt(var(y))\n\n\n[1] 0.8560768\n\n\nCode\nsd(y)\n\n\n[1] 0.8560768"
  },
  {
    "objectID": "assignment_1_files/Lab01.html#visualization-using-r-graphics-without-packages",
    "href": "assignment_1_files/Lab01.html#visualization-using-r-graphics-without-packages",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "Code\nx=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\n\n\n\n\n\n\nCode\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\n\n\n\n\n\n\nCode\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\n\n\n\n\n\n\nCode\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\n\nquartz_off_screen \n                2 \n\n\nCode\nplot(x,y,pch=20, col=\"forestgreen\")\n\n\n\n\n\n\n\n\n\nCode\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nCode\nx=1:10\nx\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nCode\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "assigned_reading.html",
    "href": "assigned_reading.html",
    "title": "Assigned Reading- Breiman",
    "section": "",
    "text": "Reading Material\n{{&lt; fa file-pdf &gt;}} Click here to download/view the article: Breiman - Statistical Modeling"
  },
  {
    "objectID": "assigned_reading.html#section",
    "href": "assigned_reading.html#section",
    "title": "Assigned Reading- Breiman",
    "section": "",
    "text": "Reading Material\n{{&lt; fa file-pdf &gt;}} Click here to download/view the article: Breiman - Statistical Modeling"
  },
  {
    "objectID": "assigned_reading.html#section-1",
    "href": "assigned_reading.html#section-1",
    "title": "Assigned Reading- Breiman",
    "section": "",
    "text": "Notes & Review\nThere are two cultures in statistical modeling. The first is dependent on stochastic data models, and the other on algorithmic models. Breiman (2001) argues that algorithmic models can be utilized on large, complex data sets to provide more accurate information. Consequently, the author suggests a shift away from a heavy dependence on stochastic data models.\nIn data analysis, Breiman (2001) identifies two primary goals: prediction and information. Prediction involves estimating the responses of future input variables. The second goal, information, entails extracting insight into how nature associates response variables with input variables. In the data modeling culture, a stochastic model is assumed to exist inside the ‚Äúblack box.‚Äù The values of the parameters are estimated from the data, and the model is then used for information or prediction. This model is typically validated using goodness-of-fit tests and residual examination. Breiman (2001) notes that approximately 98% of statisticians focus their efforts on this stochastic data model method.\nIn contrast, the algorithmic model considers the interior of the black box to be complex and unknown. This approach focuses on finding an algorithm that operates on input¬†x¬†to predict the responses of¬†y. The validation of this model is measured by its predictive accuracy. According to Breiman (2001), only about 2% of the statistical community utilizes this method.\nBreiman makes three primary arguments. For one, traditional data modeling has led to irrelevant theory and questionable conclusions. It has kept statisticians from using more suitable algorithmic models. Lastly, it has prevented statisticians from working on exciting new problems.\nBreiman uses the Ozone project to illustrate the limitations of traditional modeling. In the Ozone project, the EPA sought to predict ozone levels 12 hours in advance to provide more timely public alerts. Ozone levels were known to be influenced by meteorological conditions in prior days, making the task dependent on complex environmental patterns. Researchers attempted to build a predictive model that would use available variables to forecast the following day‚Äôs ozone levels. To complete this, the first 5-years of data were used as a training set and the last two were used as a test set. Large linear regressions were run and variable selections were conducted. Ultimately, the project failed. Breiman presents this failure as evidence of the limitations of traditional data modeling approaches when applied to complex, real-world prediction problems.\nBreiman uses an example from an EPA project to illustrate how traditional data modeling approaches struggle with high-dimensional data. The goal of the chlorine project was to predict whether a chemical compound contains chlorine using its mass spectrum. Mass spectra provide information about a compound‚Äôs structure, but interpreting them typically requires substantial effort from experienced chemists, making the process costly and time-consuming. The project resulted in a database containing known chemical structures and mass spectra for approximately 30,000 compounds. Researchers attempted to use this database to automate the classification of compounds as chlorine or non-chlorine. Traditional statistical methods, including linear and quadratic discriminant analysis, were applied but proved difficult to adapt due to the high dimensional of the predictor variables. When algorithmic modeling approaches, particularly decision trees, were used instead, the classification accuracy improved substantially, reaching about 95% for both chlorine and non-chlorine compounds.\nStatisticians focus too much on data models. Nature is complex, and it is unrealistic to believe that a statistician can fully and accurately specify the parameters and mechanisms that generate real-world data. Breiman uses an example to show that once a data-generating model is assumed, it becomes easy to derive hypothesis tests, goodness-of-fit measures, and confidence intervals. This mathematical convenience makes data models attractive. However, significance levels for coefficients are often derived with little consideration of whether the data could plausibly have been generated by a linear model. As a result, measures such as R squared are computed without adequately checking model validity, which can lead to incorrect conclusions.\nCurrent practice for checking model fit relies on goodness-of-fit measures and residual analysis. Breiman conducts a simulation study using regression with controlled departures from linearity in seven dimensions. Standard goodness-of-fit tests did not reject linearity until the departures were extreme. Tinkering with the model based on the observed data invalidates the use of goodness-of-fit tests. Residual analysis is also unreliable, as it cannot uncover lack of fit in more than four or five dimensions. In higher dimensions, interactions among variables can produce passable residual plots for a variety of models. Thus, an acceptable residual plot does not imply that the model provides a good representation of the data. Breiman argues that evaluating how well a model fits the observed data is less important than assessing its predictive performance.\nThe goal of statisticians is often to uncover the underlying mechanisms that produce the data. Data modeling attempts to achieve this by providing a simple and interpretable description of the relationship between input and response variables. However, the same dataset can be well fit by multiple models, each of which offers a different interpretation of the underlying mechanism.\nTo see how well a model emulates nature‚Äôs box, put¬†XX¬†into nature‚Äôs box and observe the output¬†YY. Then put the same¬†XX¬†into the model box and get¬†Y^Y^. Comparing the two outputs shows how well the model emulates nature. The extent to which a model box reproduces nature‚Äôs box measures how well the model captures the natural phenomenon generating the data. Breiman also discusses the bias of using too many parameters, which can overfit the data and give a biased estimate of predictive accuracy. Bias can be reduced using methods such as cross-validation. For large datasets, a separate test set can be held out. Breiman argues for the need to evaluate models using¬†predictive accuracy¬†rather than solely relying on goodness-of-fit.\nThere is a new research community that focuses on neural networks and decision trees. The goal of this field is¬†predictive accuracy. Researchers began developing tools for complex prediction problems where traditional data models were clearly inadequate, such as speech recognition, image recognition, nonlinear time series prediction, handwriting recognition, and prediction in financial markets.\nThe approach in this field rarely uses data models. Instead, it treats nature as a black box, whose workings are complex, mysterious, and partly unknowable. The problem is to find an algorithm such that, for future¬†XX¬†in a test set,¬†f(X)f(X)¬†is a good predictor of¬†YY. This theory shifts the focus from modeling the data to studying the properties of algorithms, emphasizing their strengths as predictors, their iterative convergence, and what contributes to their predictive accuracy. The only assumption is that the data are drawn from an unknown multivariate distribution.\nThere have been three important lessons to learn from the field on machine learning: Rashomon; the multiplicity of good models; Occam: the conflict between simplicity and accuracy; & Bellman: dimensionality-curse or blessing.\nThe Rashomon Effect refers to the existence of a multitude of different functions within a class of functions that achieve approximately the same minimum error rate. For example, suppose there are 300 variables and we want the best 5-variable linear regression. There are about 140,000 subsets in competition. If we select the model with the lowest residual sum of squares, or, when using a test set, the lowest prediction error, there may still be many subsets within 1% of the minimum RSS.\nThe Rashomon Effect also occurs with decision trees and neural networks. For trees, even a slight perturbation of the training set‚Äîsuch as removing only 2‚Äì3% of the data‚Äîcan produce a tree that is quite different from the original, yet has nearly the same test set error. This instability arises when many different models are crowded together with similar training or test errors. Small changes in the data or model construction can cause a jump from one model to another. These models are close in error but distant in form.\nOccam‚Äôs Razor states that simpler models are better. In prediction, simplicity and accuracy can clash.\nInstead of using a single tree, a forest of trees is grown. An input¬†xx¬†is passed down each tree, and each tree votes for a predicted class. The class receiving the most votes is the forest‚Äôs prediction. There are well-known methods for growing forests, such as perturbing the training set, growing a tree on the perturbed data, perturbing the data again, and growing another tree. Breiman‚Äôs preferred method is random forests, in which successive decision trees are grown by introducing randomness into their construction through random selection.\nComparing forests to single trees, forests have a significantly lower error rate because they average the predictions of many unstable trees, reducing variance while maintaining flexibility.\nThe Statlog Project compared 18 different classifiers. Random forests ranked first overall, with an average rank of 1, demonstrating their strong predictive performance.\nOccam‚Äôs dilemma is that forests are excellent predictors, but their prediction mechanisms are difficult to interpret. High predictive accuracy generally requires complex methods, while simple and interpretable functions tend to be less accurate. Breiman argues that the soundest path is to prioritize predictive accuracy. In goal-oriented statistics, Occam‚Äôs dilemma does not exist.\nBellman and the curse of dimensionality. Rejects this claim. Argues that dimensionality means more informatioin for predcition,\nDisclosure: I utilized Gemini 3 Flash for technical assistance with Quarto website formatting and file structure; however, all intellectual content and writing in the ‚ÄúNotes & Review‚Äù section are my own original work."
  },
  {
    "objectID": "assignment_1_files/Lab02.html",
    "href": "assignment_1_files/Lab02.html",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "(Adapted from ISLR Chapter 3 Lab: Introduction to R)\n\n\n\n\nCode\nA=matrix(1:16,4,4)\nA\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\n\nCode\nA[2,3]\n\n\n[1] 10\n\n\nCode\nA[c(1,3),c(2,4)]\n\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\n\nCode\nA[1:3,2:4]\n\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\n\nCode\nA[1:2,]\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\n\nCode\nA[,1:2]\n\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\n\nCode\nA[1,]\n\n\n[1]  1  5  9 13\n\n\nCode\nA[-c(1,3),] # What does -c() do?\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\n\nCode\nA[-c(1,3),-c(1,3,4)]\n\n\n[1] 6 8\n\n\nCode\ndim(A) # Dimensions\n\n\n[1] 4 4\n\n\n\n\n\n\n\nCode\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\") \nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\") # read csv file\n# Which function reads data faster?\n\n# Try using this simple method\n# time1 = proc.time()\n# Auto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\n# proc.time()-time1\n\n# Check on data\ndim(Auto)\n\n\n[1] 397   9\n\n\nCode\nAuto[1:4,] # select rows\n\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\n\nCode\nAuto=na.omit(Auto)\ndim(Auto) # Notice the difference?\n\n\n[1] 397   9\n\n\nCode\nnames(Auto)\n\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\"        \n\n\n\n\n\n\n\nCode\nAuto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n\n[1] 397   9\n\n\n\n\n\n\n\nCode\n# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\n\n\n\n\n\n\nCode\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\n\n\nCode\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\n\n\nCode\nplot(cylinders, mpg, col=\"red\")\n\n\n\n\n\n\n\n\n\nCode\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\n\n\n\n\n\n\nCode\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\n\n\n\n\n\n\nCode\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\n\n\n\n\n\n\nCode\nhist(mpg)\n\n\n\n\n\n\n\n\n\nCode\nhist(mpg,col=2)\n\n\n\n\n\n\n\n\n\nCode\nhist(mpg,col=2,breaks=15)\n\n\n\n\n\n\n\n\n\nCode\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\n\n\n\n\n\n\nCode\nplot(horsepower,mpg)\n\n\n\n\n\n\n\n\n\nCode\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\n\nCode\nsummary(mpg)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60 \n\n\n\n\n\n\n\nCode\nptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\n\n\nThe downloaded binary packages are in\n    /var/folders/th/bpf3s19x3bddppwycgt21qgw0000gn/T//Rtmp2eRVGb/downloaded_packages\n\n\nCode\nlapply(ptbu, require, character.only = TRUE)\n\n\nLoading required package: MASS\n\n\nLoading required package: ISLR\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\n\nCode\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n\nCode\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\n\nCode\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\n\nCode\nsummary(lm.fit)\n\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nnames(lm.fit)\n\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n\nCode\ncoef(lm.fit)\n\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\n\nCode\nconfint(lm.fit)\n\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\n\nCode\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\n\nCode\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n\nCode\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\n\n\n\n\n\n\nCode\nplot(lstat,medv,col=\"red\")\n\n\n\n\n\n\n\n\n\nCode\nplot(lstat,medv,pch=16)\n\n\n\n\n\n\n\n\n\nCode\nplot(lstat,medv,pch=\"+\")\n\n\n\n\n\n\n\n\n\nCode\nplot(1:20,1:20,pch=1:20)\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\n\n\n\n\n\n\nCode\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n\n375 \n375 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nlibrary(car)\n\n\nWarning: package 'car' was built under R version 4.5.2\n\n\nLoading required package: carData\n\n\nWarning: package 'carData' was built under R version 4.5.2\n\n\nCode\nvif(lm.fit)\n\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\n\nCode\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nlm.fit1=update(lm.fit, ~.-age)\n\n\n\n\n\n\n\nCode\nlm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\n\n\n\n\n\n\nCode\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nsummary(lm(medv~log(rm),data=Boston))\n\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\nCode\n# fix(Carseats)\nnames(Carseats)\n\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\n\nCode\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\n\n\n\n\n\nCode\nsummary(lm(medv~lstat*age,data=Boston))\n\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "assignment_1_files/Lab02.html#indexing-data-using",
    "href": "assignment_1_files/Lab02.html#indexing-data-using",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Code\nA=matrix(1:16,4,4)\nA\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\n\nCode\nA[2,3]\n\n\n[1] 10\n\n\nCode\nA[c(1,3),c(2,4)]\n\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\n\nCode\nA[1:3,2:4]\n\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\n\nCode\nA[1:2,]\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\n\nCode\nA[,1:2]\n\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\n\nCode\nA[1,]\n\n\n[1]  1  5  9 13\n\n\nCode\nA[-c(1,3),] # What does -c() do?\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\n\nCode\nA[-c(1,3),-c(1,3,4)]\n\n\n[1] 6 8\n\n\nCode\ndim(A) # Dimensions\n\n\n[1] 4 4"
  },
  {
    "objectID": "assignment_1_files/Lab02.html#loading-data-from-github-remote",
    "href": "assignment_1_files/Lab02.html#loading-data-from-github-remote",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Code\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\") \nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\") # read csv file\n# Which function reads data faster?\n\n# Try using this simple method\n# time1 = proc.time()\n# Auto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\n# proc.time()-time1\n\n# Check on data\ndim(Auto)\n\n\n[1] 397   9\n\n\nCode\nAuto[1:4,] # select rows\n\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\n\nCode\nAuto=na.omit(Auto)\ndim(Auto) # Notice the difference?\n\n\n[1] 397   9\n\n\nCode\nnames(Auto)\n\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\""
  },
  {
    "objectID": "assignment_1_files/Lab02.html#load-data-from-islr-website",
    "href": "assignment_1_files/Lab02.html#load-data-from-islr-website",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Code\nAuto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n\n[1] 397   9"
  },
  {
    "objectID": "assignment_1_files/Lab02.html#additional-graphical-and-numerical-summaries",
    "href": "assignment_1_files/Lab02.html#additional-graphical-and-numerical-summaries",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Code\n# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\n\n\n\n\n\n\nCode\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\n\n\nCode\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\n\n\nCode\nplot(cylinders, mpg, col=\"red\")\n\n\n\n\n\n\n\n\n\nCode\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\n\n\n\n\n\n\nCode\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\n\n\n\n\n\n\nCode\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\n\n\n\n\n\n\nCode\nhist(mpg)\n\n\n\n\n\n\n\n\n\nCode\nhist(mpg,col=2)\n\n\n\n\n\n\n\n\n\nCode\nhist(mpg,col=2,breaks=15)\n\n\n\n\n\n\n\n\n\nCode\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\n\n\n\n\n\n\nCode\nplot(horsepower,mpg)\n\n\n\n\n\n\n\n\n\nCode\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\n\nCode\nsummary(mpg)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60"
  },
  {
    "objectID": "assignment_1_files/Lab02.html#linear-regression",
    "href": "assignment_1_files/Lab02.html#linear-regression",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Code\nptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\n\n\nThe downloaded binary packages are in\n    /var/folders/th/bpf3s19x3bddppwycgt21qgw0000gn/T//Rtmp2eRVGb/downloaded_packages\n\n\nCode\nlapply(ptbu, require, character.only = TRUE)\n\n\nLoading required package: MASS\n\n\nLoading required package: ISLR\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\n\nCode\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n\nCode\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\n\nCode\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\n\nCode\nsummary(lm.fit)\n\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nnames(lm.fit)\n\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n\nCode\ncoef(lm.fit)\n\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\n\nCode\nconfint(lm.fit)\n\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\n\nCode\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\n\nCode\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n\nCode\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\n\n\n\n\n\n\nCode\nplot(lstat,medv,col=\"red\")\n\n\n\n\n\n\n\n\n\nCode\nplot(lstat,medv,pch=16)\n\n\n\n\n\n\n\n\n\nCode\nplot(lstat,medv,pch=\"+\")\n\n\n\n\n\n\n\n\n\nCode\nplot(1:20,1:20,pch=1:20)\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\n\n\n\n\n\n\nCode\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n\n375 \n375"
  },
  {
    "objectID": "assignment_1_files/Lab02.html#multiple-linear-regression",
    "href": "assignment_1_files/Lab02.html#multiple-linear-regression",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Code\nlm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nlibrary(car)\n\n\nWarning: package 'car' was built under R version 4.5.2\n\n\nLoading required package: carData\n\n\nWarning: package 'carData' was built under R version 4.5.2\n\n\nCode\nvif(lm.fit)\n\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\n\nCode\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nlm.fit1=update(lm.fit, ~.-age)"
  },
  {
    "objectID": "assignment_1_files/Lab02.html#non-linear-transformations-of-the-predictors",
    "href": "assignment_1_files/Lab02.html#non-linear-transformations-of-the-predictors",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Code\nlm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\n\n\n\n\n\n\nCode\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nsummary(lm(medv~log(rm),data=Boston))\n\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "assignment_1_files/Lab02.html#qualitative-predictors",
    "href": "assignment_1_files/Lab02.html#qualitative-predictors",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Code\n# fix(Carseats)\nnames(Carseats)\n\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\n\nCode\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1"
  },
  {
    "objectID": "assignment_1_files/Lab02.html#interaction-terms-including-interaction-and-single-effects",
    "href": "assignment_1_files/Lab02.html#interaction-terms-including-interaction-and-single-effects",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Code\nsummary(lm(medv~lstat*age,data=Boston))\n\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "assignment_1_files/TEDS2016.html",
    "href": "assignment_1_files/TEDS2016.html",
    "title": "Assignment 1 TEDS2016",
    "section": "",
    "text": "Code\nlibrary(haven)\nTEDS_2016&lt;-\n  read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\nlibrary(labelled)"
  },
  {
    "objectID": "assignment_1_files/TEDS2016.html#exploratory-analysis-on-data",
    "href": "assignment_1_files/TEDS2016.html#exploratory-analysis-on-data",
    "title": "Assignment 1 TEDS2016",
    "section": "Exploratory Analysis on Data",
    "text": "Exploratory Analysis on Data\n\n\nCode\ndim(TEDS_2016)\n\n\n[1] 1690   54\n\n\nCode\nstr(TEDS_2016)\n\n\ntibble [1,690 √ó 54] (S3: tbl_df/tbl/data.frame)\n $ District       : dbl+lbl [1:1690] 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201...\n   ..@ label       : chr \"District\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:73] 201 401 501 502 701 702 703 704 801 802 ...\n   .. ..- attr(*, \"names\")= chr [1:73] \"Yi Lan County Single District\" \"Hsinchu County Single District\" \"Miaoli County 1st District\" \"Miaoli County 2nd District\" ...\n $ Sex            : dbl+lbl [1:1690] 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1,...\n   ..@ label       : chr \"Sex\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:2] 1 2\n   .. ..- attr(*, \"names\")= chr [1:2] \"Male\" \"Female\"\n $ Age            : dbl+lbl [1:1690] 4, 2, 5, 4, 5, 5, 5, 4, 5, 4, 5, 1, 5, 3, 4, 5, 4, 5,...\n   ..@ label       : chr \"Age\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:5] 1 2 3 4 5\n   .. ..- attr(*, \"names\")= chr [1:5] \"20-29\" \"30-39\" \"40-49\" \"50-59\" ...\n $ Edu            : dbl+lbl [1:1690] 4, 5, 5, 2, 1, 2, 1, 5, 1, 1, 1, 2, 1, 5, 5, 1, 3, 4,...\n   ..@ label       : chr \"Education\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:6] 1 2 3 4 5 9\n   .. ..- attr(*, \"names\")= chr [1:6] \"Below elementary school\" \"Junior high school\" \"Senior high school\" \"College\" ...\n $ Arear          : dbl+lbl [1:1690] 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\n   ..@ label       : chr \"Area\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:6] 1 2 3 4 5 6\n   .. ..- attr(*, \"names\")= chr [1:6] \"Taipei, New Taipei, Keelung and Yi Lan\" \"Taoyuan, Hsinchu and Miaoli\" \"Taichung, Changhua and Nantou\" \"Yunlin, Chiayi and Tainan\" ...\n $ Career         : dbl+lbl [1:1690] 1, 2, 1, 4, 3, 2, 4, 1, 4, 3, 3, 5, 5, 4, 1, 5, 2, 2,...\n   ..@ label       : chr \"Occupations5\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:5] 1 2 3 4 5\n   .. ..- attr(*, \"names\")= chr [1:5] \"Hight-class WHITE COLLAR\" \"Low-class WHITE COLLAR\" \"FARMER\" \"WORKER\" ...\n $ Career8        : dbl+lbl [1:1690] 1, 3, 1, 4, 5, 7, 4, 2, 4, 5, 5, 7, 7, 7, 2, 7, 3, 1,...\n   ..@ label       : chr \"Occupation8\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:8] 1 2 3 4 5 6 7 8\n   .. ..- attr(*, \"names\")= chr [1:8] \"Civil servants\" \"Managers and  Professionals (priv.)\" \"CLERKS (priv.)\" \"Labor (priv.)\" ...\n $ Ethnic         : dbl+lbl [1:1690] 1, 2, 2, 1, 9, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 9, 2, 2,...\n   ..@ label       : chr \"Ethnic\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:4] 1 2 3 9\n   .. ..- attr(*, \"names\")= chr [1:4] \"Taiwanese\" \"Both\" \"Chinese\" \"Noresponse\"\n $ Party          : dbl+lbl [1:1690] 25, 25,  3, 25, 25,  6, 25, 24, 25, 25,  6,  5, 25,  ...\n   ..@ label       : chr \"Party Preference\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:26] 1 2 3 4 5 6 7 8 9 10 ...\n   .. ..- attr(*, \"names\")= chr [1:26] \"Strongly support KMT\" \"Somewhat support KMT\" \"Lean to KMT\" \"Somewhat lean to KMT\" ...\n $ PartyID        : dbl+lbl [1:1690] 9, 9, 1, 9, 9, 2, 9, 6, 9, 9, 2, 2, 9, 1, 1, 9, 9, 9,...\n   ..@ label       : chr \"Party Identification\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:7] 1 2 3 4 5 6 9\n   .. ..- attr(*, \"names\")= chr [1:7] \"KMT\" \"DPP\" \"NP\" \"PFP\" ...\n $ Tondu          : dbl+lbl [1:1690] 3, 5, 3, 5, 9, 4, 9, 6, 9, 9, 5, 5, 9, 5, 4, 9, 9, 4,...\n   ..@ label       : chr \"Position on unification and independence\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:7] 1 2 3 4 5 6 9\n   .. ..- attr(*, \"names\")= chr [1:7] \"Immediate unification\" \"Maintain the status quo,move toward unification\" \"Maintain the status quo, decide either unification or independence\" \"Maintain the status quo forever\" ...\n $ Tondu3         : dbl+lbl [1:1690] 2, 3, 2, 3, 9, 2, 9, 3, 9, 9, 3, 3, 9, 3, 2, 9, 9, 2,...\n   ..@ label       : chr \"3 categories of TONDU\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:4] 1 2 3 9\n   .. ..- attr(*, \"names\")= chr [1:4] \"Unification\" \"Maintain the status quo\" \"Independence\" \"Nonresponse\"\n $ nI2            : dbl+lbl [1:1690]  3, 98, 98,  3, 98, 98, 98,  3, 98,  1,  2, 98, 98,  ...\n   ..@ label       : chr \"Who is the current the premier of our country?\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:5] 1 2 3 95 98\n   .. ..- attr(*, \"names\")= chr [1:5] \"Correct\" \"Incorrect\" \"I know but can't remember the name\" \"Refuse to answer\" ...\n $ votetsai       : num [1:1690] NA 1 0 NA NA 1 1 1 1 NA ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ green          : num [1:1690] 0 0 0 0 0 1 0 1 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ votetsai_nm    : num [1:1690] NA 1 0 NA NA 1 1 1 1 NA ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ votetsai_all   : num [1:1690] 0 1 0 0 0 1 1 1 1 NA ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Independence   : num [1:1690] 0 1 0 1 0 0 0 1 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Unification    : num [1:1690] 0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ sq             : num [1:1690] 1 0 1 0 0 1 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Taiwanese      : num [1:1690] 1 0 0 1 0 1 0 1 1 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ edu            : num [1:1690] 4 5 5 2 1 2 1 5 1 1 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ female         : num [1:1690] 1 1 0 0 1 1 0 1 1 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ whitecollar    : num [1:1690] 1 1 1 0 0 1 0 1 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ lowincome      : num [1:1690] 4 4 5 4 3 5 2 5 5 5 ...\n  ..- attr(*, \"label\")= chr \"How serious do you think low income of salaryman?\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ income         : num [1:1690] 8 7 8 5 5.5 9 1 10 2 5.5 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ income_nm      : num [1:1690] 8 7 8 5 NA 9 1 10 2 NA ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ age            : num [1:1690] 59 39 63 55 76 64 75 54 64 59 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ KMT            : num [1:1690] 0 0 1 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ DPP            : num [1:1690] 0 0 0 0 0 1 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ npp            : num [1:1690] 0 0 0 0 0 0 0 1 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ noparty        : num [1:1690] 1 1 0 1 1 0 1 0 1 1 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ pfp            : num [1:1690] 0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ South          : num [1:1690] 0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ north          : num [1:1690] 1 1 1 1 1 1 1 1 1 1 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Minnan_father  : num [1:1690] 1 1 1 1 1 1 1 1 1 1 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Mainland_father: num [1:1690] 0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Econ_worse     : num [1:1690] 0 0 1 1 0 1 1 1 1 1 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Inequality     : num [1:1690] 1 1 1 1 0 1 0 1 1 1 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ inequality5    : num [1:1690] 4 5 5 5 3 5 3 5 5 5 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ econworse5     : num [1:1690] 3 3 4 5 3 4 4 5 5 5 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Govt_for_public: num [1:1690] 1 1 1 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ pubwelf5       : num [1:1690] 5 5 4 1 3 2 2 1 3 2 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Govt_dont_care : num [1:1690] 0 0 1 1 0 1 1 1 0 1 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ highincome     : num [1:1690] 1 1 1 1 NA 1 0 1 0 NA ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ votekmt        : num [1:1690] 0 0 1 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ votekmt_nm     : num [1:1690] NA 0 1 NA NA 0 0 0 0 NA ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Blue           : num [1:1690] 0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Green          : num [1:1690] 0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ No_Party       : num [1:1690] 0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ voteblue       : num [1:1690] 0 0 1 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ voteblue_nm    : num [1:1690] NA 0 1 NA NA 0 0 0 0 NA ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ votedpp_1      : num [1:1690] NA 1 0 NA NA 1 1 1 1 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ votekmt_1      : num [1:1690] NA 0 1 NA NA 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n\n\nCode\nhead(TEDS_2016)\n\n\n# A tibble: 6 √ó 54\n  District      Sex     Age     Edu     Arear   Career  Career8 Ethnic  Party   \n  &lt;dbl+lbl&gt;     &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+lb&gt;\n1 201 [Yi Lan ‚Ä¶ 2 [Fem‚Ä¶ 4 [50-‚Ä¶ 4 [Col‚Ä¶ 1 [Tai‚Ä¶ 1 [Hig‚Ä¶ 1 [Civ‚Ä¶ 1 [Tai‚Ä¶ 25 [Neu‚Ä¶\n2 201 [Yi Lan ‚Ä¶ 2 [Fem‚Ä¶ 2 [30-‚Ä¶ 5 [Abo‚Ä¶ 1 [Tai‚Ä¶ 2 [Low‚Ä¶ 3 [CLE‚Ä¶ 2 [Bot‚Ä¶ 25 [Neu‚Ä¶\n3 201 [Yi Lan ‚Ä¶ 1 [Mal‚Ä¶ 5 [Abo‚Ä¶ 5 [Abo‚Ä¶ 1 [Tai‚Ä¶ 1 [Hig‚Ä¶ 1 [Civ‚Ä¶ 2 [Bot‚Ä¶  3 [Lea‚Ä¶\n4 201 [Yi Lan ‚Ä¶ 1 [Mal‚Ä¶ 4 [50-‚Ä¶ 2 [Jun‚Ä¶ 1 [Tai‚Ä¶ 4 [WOR‚Ä¶ 4 [Lab‚Ä¶ 1 [Tai‚Ä¶ 25 [Neu‚Ä¶\n5 201 [Yi Lan ‚Ä¶ 2 [Fem‚Ä¶ 5 [Abo‚Ä¶ 1 [Bel‚Ä¶ 1 [Tai‚Ä¶ 3 [FAR‚Ä¶ 5 [FAR‚Ä¶ 9 [Nor‚Ä¶ 25 [Neu‚Ä¶\n6 201 [Yi Lan ‚Ä¶ 2 [Fem‚Ä¶ 5 [Abo‚Ä¶ 2 [Jun‚Ä¶ 1 [Tai‚Ä¶ 2 [Low‚Ä¶ 7 [Hou‚Ä¶ 1 [Tai‚Ä¶  6 [Som‚Ä¶\n# ‚Ñπ 45 more variables: PartyID &lt;dbl+lbl&gt;, Tondu &lt;dbl+lbl&gt;, Tondu3 &lt;dbl+lbl&gt;,\n#   nI2 &lt;dbl+lbl&gt;, votetsai &lt;dbl&gt;, green &lt;dbl&gt;, votetsai_nm &lt;dbl&gt;,\n#   votetsai_all &lt;dbl&gt;, Independence &lt;dbl&gt;, Unification &lt;dbl&gt;, sq &lt;dbl&gt;,\n#   Taiwanese &lt;dbl&gt;, edu &lt;dbl&gt;, female &lt;dbl&gt;, whitecollar &lt;dbl&gt;,\n#   lowincome &lt;dbl&gt;, income &lt;dbl&gt;, income_nm &lt;dbl&gt;, age &lt;dbl&gt;, KMT &lt;dbl&gt;,\n#   DPP &lt;dbl&gt;, npp &lt;dbl&gt;, noparty &lt;dbl&gt;, pfp &lt;dbl&gt;, South &lt;dbl&gt;, north &lt;dbl&gt;,\n#   Minnan_father &lt;dbl&gt;, Mainland_father &lt;dbl&gt;, Econ_worse &lt;dbl&gt;, ‚Ä¶\n\n\nCode\nsummary(TEDS_2016) #variables with large quantities of NA's: votekm-1, votedpp_1,votekmt_nm,highincome,income_nm,votesai_all,votetsai,votesai_nm\n\n\n    District         Sex             Age           Edu            Arear      \n Min.   : 201   Min.   :1.000   Min.   :1.0   Min.   :1.000   Min.   :1.000  \n 1st Qu.:1401   1st Qu.:1.000   1st Qu.:2.0   1st Qu.:2.000   1st Qu.:1.000  \n Median :6406   Median :1.000   Median :3.0   Median :3.000   Median :3.000  \n Mean   :4661   Mean   :1.486   Mean   :3.3   Mean   :3.334   Mean   :2.744  \n 3rd Qu.:6604   3rd Qu.:2.000   3rd Qu.:5.0   3rd Qu.:5.000   3rd Qu.:4.000  \n Max.   :6806   Max.   :2.000   Max.   :5.0   Max.   :9.000   Max.   :6.000  \n                                                                             \n     Career         Career8          Ethnic          Party      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   : 1.00  \n 1st Qu.:1.000   1st Qu.:2.000   1st Qu.:1.000   1st Qu.: 5.00  \n Median :2.000   Median :4.000   Median :1.000   Median : 7.00  \n Mean   :2.683   Mean   :3.811   Mean   :1.658   Mean   :13.02  \n 3rd Qu.:4.000   3rd Qu.:5.000   3rd Qu.:2.000   3rd Qu.:25.00  \n Max.   :5.000   Max.   :8.000   Max.   :9.000   Max.   :26.00  \n                                                                \n    PartyID          Tondu           Tondu3           nI2       \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   : 1.00  \n 1st Qu.:2.000   1st Qu.:3.000   1st Qu.:2.000   1st Qu.: 1.00  \n Median :2.000   Median :4.000   Median :2.000   Median : 3.00  \n Mean   :4.522   Mean   :4.127   Mean   :2.667   Mean   :35.13  \n 3rd Qu.:9.000   3rd Qu.:5.000   3rd Qu.:3.000   3rd Qu.:98.00  \n Max.   :9.000   Max.   :9.000   Max.   :9.000   Max.   :98.00  \n                                                                \n    votetsai          green         votetsai_nm      votetsai_all   \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :0.0000   Median :1.0000   Median :1.0000  \n Mean   :0.6265   Mean   :0.3781   Mean   :0.6265   Mean   :0.5478  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :429                       NA's   :429      NA's   :248     \n  Independence     Unification           sq           Taiwanese     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :1.0000   Median :1.0000  \n Mean   :0.2888   Mean   :0.1225   Mean   :0.5172   Mean   :0.6272  \n 3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n                                                                    \n      edu            female        whitecollar       lowincome    \n Min.   :1.000   Min.   :0.0000   Min.   :0.0000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:4.000  \n Median :3.000   Median :0.0000   Median :1.0000   Median :5.000  \n Mean   :3.301   Mean   :0.4864   Mean   :0.5373   Mean   :4.343  \n 3rd Qu.:5.000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:5.000  \n Max.   :5.000   Max.   :1.0000   Max.   :1.0000   Max.   :5.000  \n NA's   :10                                                       \n     income         income_nm           age              KMT        \n Min.   : 1.000   Min.   : 1.000   Min.   : 20.00   Min.   :0.0000  \n 1st Qu.: 3.000   1st Qu.: 2.000   1st Qu.: 35.00   1st Qu.:0.0000  \n Median : 5.500   Median : 5.000   Median : 49.00   Median :0.0000  \n Mean   : 5.324   Mean   : 5.281   Mean   : 49.11   Mean   :0.2296  \n 3rd Qu.: 7.000   3rd Qu.: 8.000   3rd Qu.: 61.00   3rd Qu.:0.0000  \n Max.   :10.000   Max.   :10.000   Max.   :100.00   Max.   :1.0000  \n                  NA's   :330                                       \n      DPP              npp             noparty            pfp         \n Min.   :0.0000   Min.   :0.00000   Min.   :0.0000   Min.   :0.00000  \n 1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.00000  \n Median :0.0000   Median :0.00000   Median :0.0000   Median :0.00000  \n Mean   :0.3497   Mean   :0.02544   Mean   :0.3716   Mean   :0.01893  \n 3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:1.0000   3rd Qu.:0.00000  \n Max.   :1.0000   Max.   :1.00000   Max.   :1.0000   Max.   :1.00000  \n                                                                      \n     South            north        Minnan_father    Mainland_father \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :1.0000   Median :0.0000  \n Mean   :0.4947   Mean   :0.4799   Mean   :0.7225   Mean   :0.1024  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n                                                                    \n   Econ_worse       Inequality      inequality5      econworse5   \n Min.   :0.0000   Min.   :0.0000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:4.000   1st Qu.:3.000  \n Median :1.0000   Median :1.0000   Median :5.000   Median :4.000  \n Mean   :0.5544   Mean   :0.9355   Mean   :4.495   Mean   :3.644  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:5.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :1.0000   Max.   :5.000   Max.   :5.000  \n                                                                  \n Govt_for_public     pubwelf5     Govt_dont_care     highincome    \n Min.   :0.0000   Min.   :1.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:2.000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :3.000   Median :0.0000   Median :1.0000  \n Mean   :0.4249   Mean   :2.877   Mean   :0.4988   Mean   :0.5765  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :5.000   Max.   :1.0000   Max.   :1.0000  \n                                                   NA's   :330     \n    votekmt         votekmt_nm          Blue       Green      No_Party\n Min.   :0.0000   Min.   :0.0000   Min.   :0   Min.   :0   Min.   :0  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0   1st Qu.:0   1st Qu.:0  \n Median :0.0000   Median :0.0000   Median :0   Median :0   Median :0  \n Mean   :0.2053   Mean   :0.2752   Mean   :0   Mean   :0   Mean   :0  \n 3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:0   3rd Qu.:0   3rd Qu.:0  \n Max.   :1.0000   Max.   :1.0000   Max.   :0   Max.   :0   Max.   :0  \n                  NA's   :429                                         \n    voteblue       voteblue_nm       votedpp_1        votekmt_1     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :1.0000   Median :0.0000  \n Mean   :0.2787   Mean   :0.3735   Mean   :0.5256   Mean   :0.2309  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n                  NA's   :429      NA's   :187      NA's   :187     \n\n\nCode\nsum(is.na(TEDS_2016)) #there is a total of 3008 missing values.\n\n\n[1] 3008"
  },
  {
    "objectID": "assignment_1_files/TEDS2016.html#taking-care-of-missing-values",
    "href": "assignment_1_files/TEDS2016.html#taking-care-of-missing-values",
    "title": "Assignment 1 TEDS2016",
    "section": "Taking Care of Missing Values",
    "text": "Taking Care of Missing Values\n\n\nCode\nTEDS_2016 &lt;- user_na_to_na(TEDS_2016)\n# Targeted removal: Keep rows where essential columns are not NA\nTEDS_clean &lt;- TEDS_2016[!is.na(TEDS_2016$Tondu) & !is.na(TEDS_2016$votetsai), ]"
  },
  {
    "objectID": "assignment_1_files/TEDS2016.html#exploring-the-relationships",
    "href": "assignment_1_files/TEDS2016.html#exploring-the-relationships",
    "title": "Assignment 1 TEDS2016",
    "section": "Exploring the Relationships",
    "text": "Exploring the Relationships\n\n\nCode\nTEDS_2016&lt;-user_na_to_na(TEDS_2016)\nTEDS_2016$Tondu_factor &lt;- factor(TEDS_2016$Tondu, \n                                 levels = 1:7, \n                                 labels = c(\"Unification now\", \n                                            \"Status quo, unif. in future\", \n                                            \"Status quo, decide later\", \n                                            \"Status quo forever\", \n                                            \"Status quo, indep. in future\", \n                                            \"Independence now\", \n                                            \"No response\"),\n                                 exclude = NULL)\ntondu_table &lt;- table(TEDS_2016$Tondu_factor, useNA = \"ifany\")\nprint(tondu_table)\n\n\n\n             Unification now  Status quo, unif. in future \n                          27                          180 \n    Status quo, decide later           Status quo forever \n                         546                          328 \nStatus quo, indep. in future             Independence now \n                         380                          108 \n                 No response                         &lt;NA&gt; \n                           0                          121 \n\n\nCode\nbarplot(tondu_table, \n        main = \"Attitudes Toward Unification/Independence\",\n        las = 2, # Makes axis labels perpendicular to axis\n        cex.names = 0.5) # Shrinks label size to fit"
  },
  {
    "objectID": "quantitative_methods.html",
    "href": "quantitative_methods.html",
    "title": "Quantitative Methods: Lecture Notes",
    "section": "",
    "text": "Notes and methodologies for Social Data Analytics quantitative research.\n\n\nDescription and Comparison\nInference\nAssessing Risk and Other Probability-Related Events\nIdentifying Important Relationships (Statistical Detective Work)\nLies, Damned Lies, and Statistics"
  },
  {
    "objectID": "quantitative_methods.html#section",
    "href": "quantitative_methods.html#section",
    "title": "Quantitative Methods: Lecture Notes",
    "section": "",
    "text": "Notes and methodologies for Social Data Analytics quantitative research.\n\n\nDescription and Comparison\nInference\nAssessing Risk and Other Probability-Related Events\nIdentifying Important Relationships (Statistical Detective Work)\nLies, Damned Lies, and Statistics"
  }
]