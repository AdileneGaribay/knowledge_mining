---
title: "Assigned Reading- Breiman"
editor: visual
---

## 

Reading Material

[{{< fa file-pdf >}} Click here to download/view the article: Breiman - Statistical Modeling](pdfs/breiman_statistical_modeling.pdf)

## 

Notes & Review

There are two cultures in statistical modeling. The first is dependent on stochastic data models, and the other on algorithmic models. Breiman (2001) argues that algorithmic models can be utilized on large, complex data sets to provide more accurate information. Consequently, the author suggests a shift away from a heavy dependence on stochastic data models.

In data analysis, Breiman (2001) identifies two primary goals: prediction and information. Prediction involves estimating the responses of future input variables. The second goal, information, entails extracting insight into how nature associates response variables with input variables. In the data modeling culture, a stochastic model is assumed to exist inside the "black box." The values of the parameters are estimated from the data, and the model is then used for information or prediction. This model is typically validated using goodness-of-fit tests and residual examination. Breiman (2001) notes that approximately 98% of statisticians focus their efforts on this stochastic data model method.

In contrast, the algorithmic model considers the interior of the black box to be complex and unknown. This approach focuses on finding an algorithm that operates on input x to predict the responses of y. The validation of this model is measured by its predictive accuracy. According to Breiman (2001), only about 2% of the statistical community utilizes this method.

Breiman makes three primary arguments. For one, traditional data modeling has led to irrelevant theory and questionable conclusions. It has kept statisticians from using more suitable algorithmic models. Lastly, it has prevented statisticians from working on exciting new problems.

Breiman uses the Ozone project to illustrate the limitations of traditional modeling. In the Ozone project, the EPA sought to predict ozone levels 12 hours in advance to provide more timely public alerts. Ozone levels were known to be influenced by meteorological conditions in prior days, making the task dependent on complex environmental patterns. Researchers attempted to build a predictive model that would use available variables to forecast the following day’s ozone levels. To complete this, the first 5-years of data were used as a training set and the last two were used as a test set. Large linear regressions were run and variable selections were conducted. Ultimately, the project failed. Breiman presents this failure as evidence of the limitations of traditional data modeling approaches when applied to complex, real-world prediction problems.

Breiman uses an example from an EPA project to illustrate how traditional data modeling approaches struggle with high-dimensional data. The goal of the chlorine project was to predict whether a chemical compound contains chlorine using its mass spectrum. Mass spectra provide information about a compound’s structure, but interpreting them typically requires substantial effort from experienced chemists, making the process costly and time-consuming. The project resulted in a database containing known chemical structures and mass spectra for approximately 30,000 compounds. Researchers attempted to use this database to automate the classification of compounds as chlorine or non-chlorine. Traditional statistical methods, including linear and quadratic discriminant analysis, were applied but proved difficult to adapt due to the high dimensional of the predictor variables. When algorithmic modeling approaches, particularly decision trees, were used instead, the classification accuracy improved substantially, reaching about 95% for both chlorine and non-chlorine compounds.

Statisticians focus too much on data models. Nature is complex, and it is unrealistic to believe that a statistician can fully and accurately specify the parameters and mechanisms that generate real-world data. Breiman uses an example to show that once a data-generating model is assumed, it becomes easy to derive hypothesis tests, goodness-of-fit measures, and confidence intervals. This mathematical convenience makes data models attractive. However, significance levels for coefficients are often derived with little consideration of whether the data could plausibly have been generated by a linear model. As a result, measures such as R squared are computed without adequately checking model validity, which can lead to incorrect conclusions.

Current practice for checking model fit relies on goodness-of-fit measures and residual analysis. Breiman conducts a simulation study using regression with controlled departures from linearity in seven dimensions. Standard goodness-of-fit tests did not reject linearity until the departures were extreme. Tinkering with the model based on the observed data invalidates the use of goodness-of-fit tests. Residual analysis is also unreliable, as it cannot uncover lack of fit in more than four or five dimensions. In higher dimensions, interactions among variables can produce passable residual plots for a variety of models. Thus, an acceptable residual plot does not imply that the model provides a good representation of the data. Breiman argues that evaluating how well a model fits the observed data is less important than assessing its predictive performance.

The goal of statisticians is often to uncover the underlying mechanisms that produce the data. Data modeling attempts to achieve this by providing a simple and interpretable description of the relationship between input and response variables. However, the same dataset can be well fit by multiple models, each of which offers a different interpretation of the underlying mechanism.

To see how well a model emulates nature's box, put XX into nature’s box and observe the output YY. Then put the same XX into the model box and get Y\^Y\^. Comparing the two outputs shows how well the model emulates nature. The extent to which a model box reproduces nature’s box measures how well the model captures the natural phenomenon generating the data. Breiman also discusses the bias of using too many parameters, which can overfit the data and give a biased estimate of predictive accuracy. Bias can be reduced using methods such as cross-validation. For large datasets, a separate test set can be held out. Breiman argues for the need to evaluate models using **predictive accuracy** rather than solely relying on goodness-of-fit.

There is a new research community that focuses on neural networks and decision trees. The goal of this field is **predictive accuracy**. Researchers began developing tools for complex prediction problems where traditional data models were clearly inadequate, such as speech recognition, image recognition, nonlinear time series prediction, handwriting recognition, and prediction in financial markets.

The approach in this field rarely uses data models. Instead, it treats nature as a black box, whose workings are complex, mysterious, and partly unknowable. The problem is to find an algorithm such that, for future XX in a test set, f(X)f(X) is a good predictor of YY. This theory shifts the focus from modeling the data to studying the properties of algorithms, emphasizing their strengths as predictors, their iterative convergence, and what contributes to their predictive accuracy. The only assumption is that the data are drawn from an unknown multivariate distribution.

There have been three important lessons to learn from the field on machine learning: Rashomon; the multiplicity of good models; Occam: the conflict between simplicity and accuracy; & Bellman: dimensionality-curse or blessing.

The Rashomon Effect refers to the existence of a multitude of different functions within a class of functions that achieve approximately the same minimum error rate. For example, suppose there are 300 variables and we want the best 5-variable linear regression. There are about 140,000 subsets in competition. If we select the model with the lowest residual sum of squares, or, when using a test set, the lowest prediction error, there may still be many subsets within 1% of the minimum RSS.

The Rashomon Effect also occurs with decision trees and neural networks. For trees, even a slight perturbation of the training set—such as removing only 2–3% of the data—can produce a tree that is quite different from the original, yet has nearly the same test set error. This instability arises when many different models are crowded together with similar training or test errors. Small changes in the data or model construction can cause a jump from one model to another. These models are close in error but distant in form.

Occam’s Razor states that simpler models are better. In prediction, simplicity and accuracy can clash.

Instead of using a single tree, a forest of trees is grown. An input xx is passed down each tree, and each tree votes for a predicted class. The class receiving the most votes is the forest’s prediction. There are well-known methods for growing forests, such as perturbing the training set, growing a tree on the perturbed data, perturbing the data again, and growing another tree. Breiman’s preferred method is random forests, in which successive decision trees are grown by introducing randomness into their construction through random selection.

Comparing forests to single trees, forests have a significantly lower error rate because they average the predictions of many unstable trees, reducing variance while maintaining flexibility.

The Statlog Project compared 18 different classifiers. Random forests ranked first overall, with an average rank of 1, demonstrating their strong predictive performance.

Occam’s dilemma is that forests are excellent predictors, but their prediction mechanisms are difficult to interpret. High predictive accuracy generally requires complex methods, while simple and interpretable functions tend to be less accurate. Breiman argues that the soundest path is to prioritize predictive accuracy. In goal-oriented statistics, Occam’s dilemma does not exist.

Bellman and the curse of dimensionality. Rejects this claim. Argues that dimensionality means more informatioin for predcition,

**Disclosure:** I utilized **Gemini 3 Flash** for technical assistance with Quarto website formatting and file structure; however, all intellectual content and writing in the "Notes & Review" section are my own original work.
